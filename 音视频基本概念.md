## 音视频基本概念

### 01｜基本概念：从参数的角度看视频图像

![img](https://static001.geekbang.org/resource/image/20/ca/20468627e2eccba12119a267c1abbcca.jpg?wh=2546x1406)

像素是图像的基本单元，一个个像素就组成了图像。你可以认为像素就是图像中的一个点。

图像（或视频）的分辨率是指图像的大小或尺寸。我们一般用像素个数来表示图像的尺寸。

分辨率特性的图像分辨率越高，图像就越清晰不够严谨，放大之后的图像，分辨率很高，但是它并没有很清晰。

这是因为放大的图像是通过“插值”处理得到的，而插值的像素是使用邻近像素经过插值算法计算得到的，跟实际相机拍摄的像素是不一样的，相当于“脑补”出来的像素值。因此，放大的图像还是会存在偏差，表现出来就是会模糊。

位深

一般来说，我们看到的彩色图像中，都有三个通道，这三个通道就是 R、G、B 通道。简单来说就是，彩色图像中的像素是有三个颜色值的，分别是红、绿、蓝三个值。

通常 R、G、B 各占 8 个位，也就是一个字节。8 个位能表示 256 种颜色值，那 3 个通道的话就是 256 的 3 次方个颜色值，总共是 1677 万种颜色。我们称这种图像是 8bit 图像，而这个 8bit 就是位深。

我们可以看到，**位深越大，我们能够表示的颜色值就越多**。因此，图像就可以更精确地展示你拍摄的真实世界。

所以，图像的位深越大，需要的存储空间就会越大，传输这张图像使用的流量就会越多。目前我们大多数情况下看到的图像以及视频还是 8bit 位深的。

Stride

这个 Stride 不是图像本身的属性，但是视频开发者经常会碰到，也是经常会出问题的一个东西。我们团队在工作中就多次遇到过由于客户没有处理好这个东西，从而导致播放的图像出现“花屏”的情况。

Stride 也可以称之为跨距，是图像存储的时候有的一个概念。**它指的是图像存储时内存中每行像素所占用的空间。**你可能会问，一张图像的分辨率确定了，那一行的像素值不就确定了吗？为什么还需要跨距这个东西呢？其实，为了能够快速读取一行像素，我们一般会对内存中的图像实现内存对齐，比如 16 字节对齐。

举个例子，我们现在有一张 RGB 图像，分辨率是 1278x720。我们将它存储在内存当中，一行像素需要 1278x3=3834 个字节，3834 除以 16 无法整除。因此，没有 16 字节对齐。所以如果需要对齐的话，我们需要在 3834 个字节后面填充 6 个字节，也就是 3840 个字节做 16 字节对齐，这样这幅图像的 Stride 就是 3840 了。如下图所示：

![img](https://static001.geekbang.org/resource/image/24/5e/24c1542183ce2b25116e2257c4558b5e.jpg?wh=1192x991)

这个地方你一定要注意，**每读取一行数据的时候需要跳过这多余的 6 个字节**。如果没有跳过的话，这 6 个字节的像素就会被我们误认为是下一行开始的 2 个像素（每个像素 R、G、B 各占 1 个字节，2 个像素共 6 个字节）。那这样得到的图像就完全错了，显示出来的就是“花屏”现象，屏幕会出现一条条的斜线。

所以，不管你去读取还是渲染一张图片，还是说你将这张图片存储下来，都需要设置正确的 Stride。很多时候，尤其是不规则分辨率的时候，它和图像的 Width（R、G、B 的话就是 Width x 3）是不一样的。

有的时候即便图像的 Width 是一个规则的值，比如说 1920 或者 1280 等能被 16 整除的宽度，图像存储在内存中有可能 Stride 和 Width（R、G、B 的话就是 Width x 3）也是不一样的，尤其是不同的视频解码器内部实现的不同，会导致输出的图像的 Stride 不一样。

所以，一定要在处理图片的时候注意这个 Stride 值。如果出现一条条斜线的花屏或者说解码后图像的颜色不对的情况，我们需要先确认一下这个 Stride 值对不对。

帧率

视频是由一系列图像组成的，即“连续”的一帧帧图像就可以组成视频。但事实上，视频中的图像并不是真正意义上的连续。也就是说，在 1 秒钟之内，图像的数量是有限的。只是当数量达到一定值之后，人的眼睛的灵敏度就察觉不出来了，看起来就是连续的视频了。

这个 **1 秒钟内图像的数量就是帧率**。据研究表明，一般帧率达到 10～12 帧每秒，人眼就会认为是流畅的了。

选择帧率的时候还需要考虑设备处理性能的问题，尤其是实时视频通话场景。**帧率高，代表着每秒钟处理的图像数量会很高，从而需要的设备性能就比较高。**如果是含有多个图像处理过程，比如人脸识别、美颜等算法的时候，就更需要考虑帧率大小和设备性能的问题。同样，也要考虑带宽流量的问题。帧率越大，流量也会越多，对带宽的要求也会越高。

码率

我们已经知道，视频的帧率越高，1 秒钟内的图像数据量就会越大。通常我们存储视频的时候需要对图像进行压缩之后再存储，否则视频会非常大。

那么压缩之后的视频我们一般如何描述它的大小呢？一般对于一个视频文件，我们直接看视频的大小就可以了。但是在实时通信或者直播的时候，视频是视频流的形式，我们怎么衡量呢？

这就涉及到我接下来要介绍的概念——码率。码率是指**视频在单位时间内的数据量的大小**，一般是 1 秒钟内的数据量，其单位一般是 Kb/s 或者 Mb/s。通常，**我们用压缩工具压缩同一个原始视频的时候，码率越高，图像的失真就会越小，视频画面就会越清晰。**但同时，码率越高，存储时占用的内存空间就会越大，传输时使用的流量就会越多。

![image-20230331104036565](/Users/lijiayan/Library/Application Support/typora-user-images/image-20230331104036565.png)

思考题：码率是固定的还是会变化的？如果是固定的，他是怎么做到的？

码率可以是固定的，也可以是变化的

如果是固定码率， 1. 编码后的码率小于固定码率，填充数据 2. 编码后的码率大于固定码率，丢弃细节数据，降低码率

如果每个画面的像素是固定的，相同像素，所占用空间大小一致，相同时间下，按理数据量大小/单位时间，得出码率应该相同。但是即使像素相同，码率通常却是变化，是因为这里的数据量应该是转码后的数据量吧？每帧哪怕像素相同，经过不同的压缩算法，每秒得到数据量大小不同。甚至不同画面之间，相同算法，得到的数据也是不一样的，导致码率发生变化。

### 02｜YUV & RGB：原来图像是这么丰富多彩的

图像的颜色空间

图像的颜色空间是图像和视频技术里面的一个非常重要的知识点，在图像处理、视频编码等技术中你会经常遇到这个概念。

而至于它为什么重要，其实也很好理解。在现实世界中，我们的眼睛每天看到的颜色是千变万化的。为了能够更方便地表示和处理这些颜色，不同应用领域就建立了多种不同的颜色空间，主要包括 RGB 、YUV、CMYK 、 HSI 等（后面两种和这门课程没有关系，因此这里我们不再介绍）。

RGB

RGB 相对比较简单。顾名思义，它就是指图像的每一个像素都有 R、G、B 三个值。RGB 是我们平常遇到最多的一种图像颜色空间，比如摄像头采集的原始图像就是 RGB 图像，且显示器显示的图像也是 RGB 图像。

一般来说，我们的 RGB 图像，每一个像素都是分别存储 R、G、B 三个值，且三个值依次排列存储。比如对于一张 8bit 位深的 RGB 图，每个值占用一个字节。但是，需要注意的是 RGB 图像像素中 R、G、B 三个值并不一定是按 R、G、B 顺序排列的，也有可能是 B、G、R 顺序排列。

比如 OpenCV 就经常使用 BGR 的排列方式来存储图像。所以在存储和读取 RGB 图像的时候需要稍微注意一下。RGB 和 BGR 的存储方式如下图所示：

![img](https://static001.geekbang.org/resource/image/1b/98/1b4d7c24133df818b0187fdf56898a98.jpeg?wh=1920x1080)

虽然 RGB 比较简单，同时在图像处理的时候也经常会用到。但是在视频领域，我们更多地是使用 YUV 颜色空间来表示图像的。**这是因为 R、G、B 三个颜色是有相关性的**，所以不太方便做图像压缩编码。那 YUV 究竟是怎么表示图像的呢？它又是如何存储在内存当中的呢？我们接下来就来揭开它的“面纱”。

> 相关性是指一幅图像在RGB格式的时候，将R、G、B三个通道分离开来当作图像来看的话，R、G、B三张图像内容几乎是一样的，只是颜色不同而已。具有相关性，如果拿来编码的话，三张图像同等重要，而且轮廓还差不多，但颜色又不同，因此不好编码。而YUV不同，YUV中只有Y是图像的大体轮廓，没有颜色信息。U、V是颜色信息。三张图像相互独立。并且人眼对于色彩信息相比图像的轮廓信息不敏感些。我们可以缩小U、V的大小，比如YUV420中U、V只有Y的1/4大小，本身就相比于RGB图像小了一半。然后我们编码的时候Y、U、V相关性很小，可以独立编码，也很方便。

YUV

YUV 跟 RGB 类似，也是一种颜色空间，但其种类会更多更复杂些，所以接下来我们会花大量的篇幅去讲解它。

跟 RGB 图像中 R、G、B 三个通道都跟色彩信息相关这种特点不同，**YUV 图像将亮度信息 Y 与色彩信息 U、V 分离开来。Y 表示亮度，是图像的总体轮廓，称之为 Y 分量。U、V 表示色度，主要描绘图像的色彩等信息，分别称为 U 分量和 V 分量。**这样一张图像如果没有了色度信息 U、V，只剩下亮度 Y，则依旧是一张图像，只不过是一张黑白图像。这种特点有什么好处呢？

在以前，世界上只有黑白电视机，每一帧电视画面都是黑白的，没有色彩信息。当然黑白电视机也不支持显示彩色图像。后来随着技术的发展，出现了彩色电视机，每一帧画面都是有颜色信息的，那当然我们可以使用 RGB、YUV 等颜色空间来表示一帧图像。

但是考虑到兼容老的黑白电视机，如果使用 RGB 表示图像，那么黑白电视机就没办法播放。这是因为 R、G、B 三个通道都是彩色的，而 Y、U、V 就可以。因为黑白电视机可以使用 Y 分量，Y 分量就是黑白图像，而且包含了图像的总体轮廓信息，只是没有色彩信息而已。

**YUV 主要分为 YUV 4:4:4、YUV 4:2:2、YUV 4:2:0 这几种常用的类型。**其中最常用的又是 YUV 4:2:0。这三种类型的 YUV 主要的区别就是 U、V 分量像素点的个数和采集方式。

YUV 4:4:4 就是每一个 Y 就对应一个 U 和一个 V；而 YUV 4:2:2 则是每两个 Y 共用一个 U、一个 V；YUV 4:2:0 则是每四个 Y 共用一个 U、V。我们可以通过图片来清晰地看一下三种 YUV 类型的区别。具体如下图所示：

![img](https://static001.geekbang.org/resource/image/00/ce/00d9109af3b89dbc5ecf1a81230f7bce.jpeg?wh=1920x1080)

![img](https://static001.geekbang.org/resource/image/f1/a4/f17b300f8c1yyae6eda99f1f746acea4.jpeg?wh=1920x1080)

![img](https://static001.geekbang.org/resource/image/f7/01/f745382857eefca91c1cb7151c439701.jpeg?wh=1920x1080)

YUV 4:4:4 这种类型非常简单，所以存储的方式也非常简单。

那 YUV 4:2:2 和 YUV 4:2:0 这种共用 U、V 分量的情况，应该在内存中怎么存储呢？下面我就来为你介绍一下。

YUV 存储方式主要分为两大类：Planar 和 Packed 两种。

Planar 格式的 YUV 是先连续存储所有像素点的 Y，然后接着存储所有像素点的 U，之后再存储所有像素点的 V，也可以是先连续存储所有像素点的 Y，然后接着存储所有像素点的 V，之后再存储所有像素点的 U。

Packed 格式的 YUV 是先存储完所有像素的 Y，然后 U、V 连续的交错存储。下面我们就来看看每一种 YUV 类型的存储方式是怎么样的。

1. YUV 4:4:4

这种类型的 YUV 非常简单，因为每一个 Y 对应一个 U、一个 V，所以存储的方式也非常简单。例如，4 x 2 像素的 YUV 4:4:4 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/04/53/0428a59eb7702e4feb9bafd80ac83553.jpeg?wh=1920x1080)

可以看到，YUV 4:4:4 和 RGB 图像存储之后的大小是一样的。如果是 8bit 图像，就是每一个像素点需要占用 3 个字节。

2. YUV 4:2:2

这种类型的 YUV 稍微复杂些，每左右两个像素的 Y 共用一个 U 和一个 V。

存储方式主要有以下４种类型。YU16（或者称为 I422、YUV422P）该类型是 Planar 格式，先存储完 Y，再存储 U，之后存储 V。例如，4 x 2 像素的 YU16 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/9f/45/9fb26028da7201cdbc7dd7f8dd83b145.jpeg?wh=1912x756)

YV16（YUV422P）

该类型也是 Planar 格式，先存储完 Y，再存储 V，之后存储 U。例如，4 x 2 像素的 YV16 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/6b/1e/6b1a9be5614b35bf7c96fe64ed98bc1e.jpeg?wh=1915x775)

NV16（YUV422SP）

这种类型是 Packed 格式，先存储完 Y，之后 U、V 连续交错存储。例如，4 x 2 像素的 NV16 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/79/aa/79d68992f9c21eff13d118ccabc145aa.jpeg?wh=1914x761)

NV61（YUV422SP）这种也是 Packed 格式，与 NV16 不同，这种格式是先存储完 Y，之后 V、U 连续交错存储。例如，4 x 2 像素的 NV61 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/2f/44/2f06f11d9c9dd7a49d268e325b846944.jpeg?wh=1912x755)

可以看到，4 x 2 像素的 YUV 4:2:2 只需要 16 个字节，而 RGB 图像则需要 24 个字节。也就是说，如果是 8bit 图像，那么 RGB 每一个像素需要 3 个字节，而 YUV 4:2:2 只需要 2 个字节。

2. YUV 4:2:0

这是最常见也是最常用的 YUV 类型。通常视频压缩都是 YUV 4:2:0 格式的。它是每上、下、左、右 4 个像素点共用一个 U 和一个 V。存储方式主要分为以下４种。

YU12（I420、YUV420P）这种类型是 Planar 格式，先存储完 Y，再存储 U，之后存储 V。例如，4 x 4 像素的 YU12 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/76/16/766ce511056bf5dd3399198398b80016.jpeg?wh=1920x1080)

YV12（YUV420P）

该类型也是 Planar 格式，先存储完 Y，再存储 V，之后存储 U。例如，4 x 4 像素的 YＶ12 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/88/71/880114b51649cbea783ee0be883fc971.jpeg?wh=1920x1080)

NV12（YUV420SP）这种类型是 Packed 格式，先存储完 Y，之后 U、V 连续交错存储。例如，4 x 4 像素的 NV12 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/62/45/62689d6a46003ced023b9838ca0a0845.jpeg?wh=1920x1080)

NV21（YUV420SP）这种也是 Packed 格式，与 NV12 不同，这种格式是先存储完 Y，之后 V、U 连续交错存储。例如，4 x 4 像素的 NV21 存储图如下图所示：

![img](https://static001.geekbang.org/resource/image/9e/bd/9ed80ef848c52123fb41413abfa26ebd.jpeg?wh=1920x1080)

可以看到，4 x 4 像素的 YUV 4:2:0 只需要 24 个字节相比 RGB 图像需要 48 个字节，存储的大小少了一半。也就是说，如果是 8bit 图像，RGB 每一个像素需要 3 个字节。而 YUV 4:2:0 只需要 1.5 个字节。

![img](https://static001.geekbang.org/resource/image/e7/88/e7a9fcb9f5800eab1fb0a9ed19066588.jpeg?wh=1920x1080)

RGB 和 YUV 之间的转换

我们刚才说到，一般来说，采集到的原始图像、给显示器渲染的最终图像都是 RGB 图像，但是视频编码一般用的是 YUV 图像。那么这中间一定少不了两者的相互转换。那 RGB 如何转到 YUV 呢？YUV 又如何转到 RGB 呢？

在讲转换之前，我们先了解 Color Range 这个东西。对于一个 8bit 的 RGB 图像，它的每一个 R、G、B 分量的取值按理说就是 0~255 的。但是真的是这样的吗？其实不是的。这里就涉及到 Color Range 这个概念。Color Range 分为两种，一种是 Full Range，一种是 Limited Range。Full Range 的 R、G、B 取值范围都是 0～255。而 Limited Range 的 R、G、B 取值范围是 16～235。

了解了 Color Range 之后，我们怎么规范 YUV 和 RGB 之间的互转呢？其实这也是有标准的，目前的标准主要是 BT601 和 BT709（其实还有 BT2020，我们这里不展开讲）。简单来讲，BT709 和 BT601 定义了一个 RGB 和 YUV 互转的标准规范。只有我们都按照标准来做事，那么不同厂家生产出来的产品才能对接上。BT601 是标清的标准，而 BT709 是高清的标准。

下面我们来看看这两种标准分别在 Full Range 和 Limited Range 下的 RGB 和 YUV 之间的转换公式吧。具体如下图所示：

![img](https://static001.geekbang.org/resource/image/19/72/19da3510c564b590f92cf969be01d872.jpeg?wh=1920x1080)

从上图我们可以看到每种标准下不同 Color Range 的转换公式是不同的。所以**在做 RGB 往 YUV 转换的时候我们需要知道是使用的哪个标准的哪种 Range 做的转换**，并告知对方。这样对方使用同样的标准和 Range 才可以正确的将 YUV 转换到 RGB。

如果是系统采集出来给到用户的图像就是 YUV 的话，你也需要获取这个 YUV 的存储格式、转换标准和 Color Range。这样才能保证正确地处理 YUV 和 RGB 之间的转换。

好了，这就是关于 RGB 和 YUV 颜色空间的一些知识。在最后我需要再一次强调一下前一节课中讲到的 Stride。

**在处理 YUV 图像的存储和读取的时候，也是有 Stride 这个概念的。事实上，YUV 出问题的情况更多。**在这里举一个例子，比如说一张 1283x720 的图像，一个 Y 分量存储按 16 字节对齐的话应该是每行占用 1296 个字节，所以每读取一行像素的 Y 应该是 1296 个字节，具体如下图所示。千万不要认为是 1283 个字节，不然就会出现“花屏”。这里一定要注意。

![img](https://static001.geekbang.org/resource/image/80/57/808b8310bb6dfbabfc3d6fbc2ce88057.jpeg?wh=1920x1080)

### 03｜缩放算法：如何高质量地缩放图像？

图像的缩放算法在我们的日常生活中使用非常频繁，只是可能你没有留意到。举个例子，你使用网页或者播放器看电影的时候，经常会开启全屏或者退出全屏，电影的播放画面就会变大，或者变小。这个过程里面就会用到图像的缩放算法。

事实上，只要视频的原始分辨率和播放窗口的大小不一致，就需要通过缩放处理来使得视频画面适应窗口的大小。比如说，电影分辨率是 1080P，播放器的窗口大小是 720P，则需要将电影画面从 1080P 缩小到 720P 再播放。如果你点击全屏播放，播放窗口变成了 4K，则需要将电影画面做放大处理，即放大到 4K 之后再播放。这就是一个非常典型的图像缩放的例子。

在视频开发的过程中，图像的缩放就更多了。下面我列举 3 种用到图像缩放的情形：

- 情形 1：播放窗口与原始图像分辨率不匹配的时候需要缩放。这和我刚才举的例子是一样的情况。
- 情形 2：我们在线观看视频时会有多种分辨率可以选择，即需要在一个图像分辨率的基础上缩放出多种不同尺寸的图像出来做编码，并保存多个不同分辨率的视频文件。
- 情形 3：RTC 场景，有的时候我们需要根据网络状况实时调节视频通话的分辨率。这个也是需要缩放算法来完成的。

所以，我们可以看到图像的缩放算法是一个很常用的技术，且它是非常重要的。并且，由于图像的缩放会严重影响我们视觉的主观感受，所以图像缩放算法的选择也是非常重要的。目前图像的缩放算法非常多，其中主要包括最常用的插值算法和目前比较火的 AI 超分算法。

由于目前绝大多数图像的缩放还是通过插值算法来实现的，所以我们今天主要来聊聊插值算法。插值算法有很多种，但是其基本原理都是差不多的。**它们都是使用周围已有的像素值通过一定的加权运算得到“插值像素值”。**插值算法主要包括：最近邻插值算法（Nearest）、双线性插值算法（Bilinear）、双三次插值算法（BiCubic）等。那么在一一讲解这些插值算法之前，我们不妨先来聊聊缩放算法的基本原理。

缩放的基本原理

**图像的缩放就是将原图像的已有像素经过加权运算得到目标图像的目标像素。**

什么意思呢？比如说，我们已有图像是 720P 的分辨率，称之为原图像，我们需要放大到 1080P，我们称这个 1080P 图像是目标图像。目标图像在宽度方向上放大了 1920 / 1280 = 1.5 倍，高度方向上也放大了 1080 / 720 = 1.5 倍。

那怎么通过 720P 的原图像生成 1080P 的目标图像呢？我们先将目标图像的像素位置映射到原图像的对应位置上，然后把通过插值计算得到的原图像对应位置的像素值作为目标图像相应位置的像素值。是不是有点绕？别急，下面我给你举个例子，通过它你就可以更直观地理解这句话的意思了。

比如说，1080P 目标图像中的（0，0）位置就映射到 720P 原图像的（0，0）位置，取原图像（0，0）位置的像素值作为目标图像（0，0）位置的像素值。目标图像的（1，1）位置就映射到原图像中的（0.67， 0.67）位置。最后，通过原图像已有像素插值得到（0.67，0.67）位置的像素值，并将该像素值作为目标图像（1，1）位置的像素值。

![img](https://static001.geekbang.org/resource/image/f9/9d/f94f67987afa3c6c6fca937cdf2ce49d.jpg?wh=1280x720)

好了，现在我们再回顾一下图像缩放的过程。

首先是图像放大的过程，对于 1080P 目标图像中的每一个像素点（x，y），我们只需要将它映射到 720P 原图像的（x / 1.5，y / 1.5）位置，通过原图像已有的像素值插值得到（x / 1.5，y / 1.5）的像素值就可以了。我们遍历一下目标图像中的每一个像素点位置，都能找到他们在原图像中的映射位置，并通过插值求出映射位置的像素值，这样就可以得到目标图像了，从而也就达到了放大的目的。

图像缩小的过程也是类似的。对于 360P 目标图像中的每一个像素点（x，y），我们只需要将它映射到 720P 原图像的（x * 2，y * 2）位置，通过原图像已有的像素值插值得到（x * 2，y * 2）的像素值就可以了。下面我们以更通用的表达式来表达一下缩放过程。

三种插值算法

位置映射过程很简单，主要的工作就是如何通过插值算法得到原图像映射位置的像素值。

最近邻插值

我们先来聊聊最简单的最近邻插值算法。顾名思义，最近邻插值就是：

- 首先，将目标图像中的目标像素位置，映射到原图像的映射位置。
- 然后，找到原图像中映射位置周围的 4 个像素。
- 最后，取离映射位置最近的像素点的像素值作为目标像素。

1. 比如说，我们现在要将图像从 720P 放大到 1080P。下面我们给出 1080P 目标图像中 3 个像素点（0，0）、（1，0）和（2，2）的最近邻插值过程。1080P 图像的（0，0）位置的像素，我们映射到 720P 图像的映射位置就是（0 * 1280 / 1920，0 * 720 / 1080），也就是（0，0）位置，那 1080P 的（0，0）位置的像素值直接取原图像（0，0）像素点的像素值就可以了。

2. 对于 1080P 图像的（1，0）位置的像素，我们映射到 720P 图像就是（1 * 1280 / 1920， 0 * 720 / 1080），也就是（0.67，0）位置的像素，这个像素需要插值得到。使用最近邻插值的话，（0.67，0）周围的 4 个像素分别是（0，0）、（1，0）、（0，1）和（1，1），其中距离（0.67，0）最近的位置很明显是（1，0）位置的像素。因此，我们将原图像中（1，0）位置的像素值赋值给目标图像（1，0）位置的像素点

![img](https://static001.geekbang.org/resource/image/77/d3/77989efeeb4272e0988ayy9d73e35dd3.jpg?wh=1345x754)

最近邻插值有一个明显的**缺点**，就是**它直接使用离插值位置最近的整数位置的像素作为插值像素，这样会导致相邻两个插值像素有很大的概率是相同的。**比如说，上面例子中的（1，0）位置和（2，0）位置的像素值是一样的。这样**得到的放大图像大概率会出现块状效应，而缩小图像容易出现锯齿。**这是最近邻插值的缺点。但是它也有一个优点，就是**不需要太多的计算，速度非常的快。**

**双线性插值**

双线性插值相比于最近邻插值稍微复杂一些，它也是取待插值像素周围的 4 个像素，不同的是，它需要**将这 4 个像素值通过一定的运算得到最后的插值像素。**在开始讲双线性插值的原理之前，我们先来看看双线性插值的基础，也就是线性插值的原理。

线性插值是在两个点中间的某一个位置插值得到一个新的值。线性插值认为，这个需要插值得到的点跟这两个已知点都有一定的关系，并且，待插值点与离它近的那个点更相似。因此，**线性插值是一种以距离作为权重的插值方式，距离越近权重越大，距离越远权重越小。**

比如，如下图所示，已知 （x1，y1） 与 （x2，y2）两个点，需求得 x 对应的 y 值。

![img](https://static001.geekbang.org/resource/image/42/35/42211d591931d86f0266290bf7631135.jpg?wh=1280x720)

通过线性插值方法，y 值的计算公式如下：

![img](https://static001.geekbang.org/resource/image/f1/e7/f1e4d19f792a19faa69780dcc96791e7.jpg?wh=1280x720)

**双线性插值本质上就是在两个方向上做线性插值。**由于图像是两个方向的二维数据，正好适合使用双线性插值算法。下面我们来讲讲双线性插值的具体原理。

**双线性插值其实就是三次线性插值的过程**，我们先通过两次线性插值得到两个中间值，然后再通过对这两个中间值进行一次插值得到最终的结果。如下图所示：

![img](https://static001.geekbang.org/resource/image/7f/28/7f16c2af0c92c0ce41b5a595da2d2128.jpg?wh=1280x720)

假设我们要插值求的点是 p 点，其坐标为 (x，y)。已知周围 4 个像素分别是 a、b、c、d。我们先通过 a 和 b 水平线性插值求得 m，再通过 c、d 水平插值求得 n。有了 m 和 n 之后，再通过 m、n 垂直插值求得 p 点的像素值。计算过程如下：

![img](https://static001.geekbang.org/resource/image/d4/b5/d43a159dce82ef9131c4d44a6ccd1eb5.jpg?wh=1280x720)

我们还是以 720P 放大到 1080P 为例，那么 1080P 图像中的目标像素点（2，2）的双线性插值过程是怎么样的呢？

首先，将目标像素点（2，2）映射到原图像的（1.33，1.33）位置，对应下面图中的点ｐ。找到（1.33，1.33）周围的 4 个像素（1，1）、（2，1）、（1，2）和（2，2），分别对应图中的点ａ、ｂ、ｃ和ｄ。

![img](https://static001.geekbang.org/resource/image/fc/9c/fc70e574058413ac2ddb674a78aa079c.jpg?wh=1352x758)

先通过这 4 个像素插值得到中间像素 m 和 n 的像素值。m 和 n 的坐标分别为（1.33，1）和（1.33，2）。通过上面的公式可以求得点 p（1.33，1.33）的像素值是：

![img](https://static001.geekbang.org/resource/image/6c/dc/6c59747b623681425596df5bb75c80dc.jpg?wh=1280x720)

插值求得（1.33，1.33）的值之后，将其赋值给 1080P 目标图像的（2，2）位置的像素点就可以了。这就是双线性插值的过程。双线性插值相比最近邻插值运算要多一些，因此运行时间要长一些，但是相比而言，插值之后图像效果会好于最近邻插值。

双三次插值

下面我们接着来看一种效果相比双线性插值更好一些的插值算法，就是双三次插值算法，也叫 BiCubic 插值。

在最近邻插值算法中，我们选择待插值像素周围的 4 个像素，并取离待插值像素位置最近的像素点权重为 1，其余 3 个点权重为 0。在双线性插值算法中，同样选择待插值像素周围的 4 个像素，并且每个像素以距离作为权重，距离越近权重越大，距离越远权重越小。

双三次插值算法的基本原理同前两种插值算法差不多，不同的是：

第一，**双三次插值选取的是周围的 16 个像素**，比前两种插值算法多了 3 倍。

第二，双三次插值算法的**周围像素的权重计算是使用一个特殊的 BiCubic 基函数来计算的。**

我们先通过这个 BiCubic 基函数计算得到待插值像素周围 16 个像素的权重，然后将 16 个像素加权平均就可以得到最终的待插值像素了。

![img](https://static001.geekbang.org/resource/image/db/36/db2e00c747e025346c030069239bf136.jpg?wh=1280x720)

BiCubic 基函数形式如下：

![img](https://static001.geekbang.org/resource/image/70/18/702c29205ca2542fd57a5ffc9961e118.jpg?wh=1280x717)

**双三次插值的权重值是分水平和垂直两个方向分别求得的，计算公式是一样的，都是上面这个公式。**对于周围 16 个点中的每一个点，其坐标值为（x，y），而目标图像中的目标像素在原图像中的映射坐标为 p（u，v）。那么通过上面公式可以求得其水平权重 W（u - x），垂直权重 W（v - y）。将 W（u - x）乘以 W（v - y）得到最终权重值，然后再用最终权重值乘以该点的像素值，并对 16 个点分别做同样的操作并求和，就得到待插值的像素值了。公式如下：

![img](https://static001.geekbang.org/resource/image/51/ba/5111be20665a5cdecd9e08a00b6b11ba.jpg?wh=1280x720)

我们还是以 720P 放大到 1080P 为例，那么 1080P 图像中的目标像素点（2，2）的双三次插值过程是怎么样的呢？

首先，将目标像素点（2，2）映射到原图像的（1.33，1.33）位置，对应下面图中的点ｐ。找到（1.33，1.33）周围的 16 个像素（0，0）、（1，0）一直到（3，3）。

![img](https://static001.geekbang.org/resource/image/c0/d2/c0dbfac89b7bdc8ed7d597128d6d73d2.jpg?wh=1280x720)

然后，通过 BiCubic 函数求得每一个点的水平和垂直权重。例如，（0，0）、（1，2）和（3，3）点的水平权重和垂直权重计算方式如下：

![img](https://static001.geekbang.org/resource/image/ab/9f/ab9b884abe3637a17fe3f7f112e6879f.jpg?wh=1280x720)

求出这 16 个点的水平和垂直权重，两者相乘得到最终的权重值，之后每一个像素用自己的最终权重乘以自己的像素值再求和就是（1.33，1.33）的插值像素值了。将它赋值给 1080P 图像的（2，2）像素点就可以了。

我们可以看到，**双三次插值需要计算 16 个点的权重再乘以像素值求和，相较于前面的最近邻插值和双线性插值计算量较大，但插值后的图像效果最好。**

好了，我们通过下面几幅图像来对比一下这三种插值算法的效果。我们可以看到：最近邻插值得到的图像有很多块效应，效果最差；双线性插值稍好于最近邻插值一些，但是比较模糊；双三次插值效果最好，对比度也明显好于双线性插值。

![img](https://static001.geekbang.org/resource/image/01/9d/01bbf77d405b1443c968667384d9989d.jpg?wh=1312x735)

![img](https://static001.geekbang.org/resource/image/b5/1a/b586f35fdc6a5083f4ca9a3129e7yy1a.jpg?wh=1464x990)

思考题：双三次插值需要周围 16 个像素，对于左上角的点，比如（0.5，0.5），它周围不够 16 个点怎么办呢？

可以使用第一行、第一列的像素直接填充一下就可以了，这样先补齐周围像素，然后插值，实现起来每个像素点的插值使用同一种逻辑会方便一下。不需要特殊处理。

### 05｜码流结构：原来你是这样的H264

今天，我们就接着来聊聊视频编码的码流结构，这在视频开发工作中是非常重要的。

视频编码的码流结构其实就是指视频经过编码之后得到的二进制数据是怎么组织的，换句话说，就是编码后的码流我们怎么将一帧帧编码后的图像数据分离出来，以及在二进制码流数据中，哪一块数据是一帧图像，哪一块数据是另外一帧图像。

而我们在工程开发中，需要对编码后的数据进行一些解析，以便用于之后的打包。同时我们在打包时也需要判断当前一帧图像数据它的开头和结尾在哪。这些工作的前提就是我们要清楚如何分析编码码流，那么码流结构到底是怎样的，就是当下的学习重点了。

下面我们就以 H264 编码为基础，分析一下它的码流结构，并看看它在工程中是如何应用的。

H264 的编码结构

这里有一些前置知识我们需要先了解一下。我们先一起来看几个重要的概念吧。它们之间有这样一条线索，你在接下来的学习中可以重点关注一下，对于你记忆它们也是非常有帮助的。

首先，清楚帧类型是图像的基础；其次，GOP 是以其中的 IDR 帧作为分隔点的；最后的 Slice 是我们深入帧内部以后的一个重要概念。整个过程，由浅入深。

帧类型

帧类型相信你在平时的工作中可能已经接触过一部分了，比如说我们可能经常听到视频开发工作者说 I 帧、P 帧之类的。其实在 H264 中，帧类型主要分为 3 大类，分别是 I 帧、P 帧和 B 帧。那么它们之间有什么区别呢？接下来我们就来详细聊聊。

为了减少空间冗余和时间冗余，视频编码使用了帧内预测和帧间预测技术，这些都涉及到帧。所以了解帧的类型是很有必要的。

我们知道帧内预测不需要参考已编码帧，对已编码帧是没有依赖的，并可以自行完成编码和解码。而帧间预测是需要参考已编码帧的，并对已编码帧具有依赖性。帧间预测需要参考已经编码好了的帧内编码帧或者帧间编码帧。并且，帧间编码帧又可以分为只参考前面帧的前向编码帧，和既可以参考前面帧又可以参考后面帧的双向编码帧。

为了做区分，在 H264 中，我们就将图像分为以下不同类型的帧。

![img](https://static001.geekbang.org/resource/image/6b/8d/6b908464d87e30bf977893ababf7e78d.jpg?wh=1280x392)

三种帧的示例图如下所示。例如，从左向右，第一个 B 帧参考第一个 I 帧和第一个 P 帧，第一个 P 帧只参考第一个 I 帧（箭头是从参考帧指向编码帧）。

![img](https://static001.geekbang.org/resource/image/ab/4c/ab75b04921d925f567a92796c992e54c.jpeg?wh=1920x376)

由于 P 帧和 B 帧需要参考其它帧。如果编码或者解码的过程中有一个参考帧出现错误的话，那依赖它的 P 帧和 B 帧肯定也会出现错误，而这些有问题的 P 帧（B 帧虽然也可以用来作为参考帧，但是一般用的比较少，所以这里不讨论）又会继续作为之后 P 帧或 B 帧的参考帧。因此，**错误会不断的传递。**为了避免错误的不断传递，**就有了一种特殊的 I 帧叫 IDR 帧，也叫立即刷新帧。**

H264 编码标准中规定，**IDR 帧之后的帧不能再参考 IDR 帧之前的帧。**这样，如果某一帧编码错误，之后的帧参考了这个错误帧，则也会出错。此时编码一个 IDR 帧，由于它不参考其它帧，所以只要它自己编码是正确的就不会有问题。之前有错误的帧也不会再被用作参考帧，**这样就截断了编码错误的传递**，且之后的帧就可以正常编 / 解码了。

当然，有 IDR 这种特殊的 I 帧，也就有普通的 I 帧。普通的 I 帧就是指当前帧只使用帧内预测编码，但是后面的 P 帧和 B 帧还是可以参考普通 I 帧之前的帧。但是这里我要说明一下，一般来说我们不太会使用这种普通 I 帧，大多数情况下还是直接使用 IDR 帧，尤其是在流媒体场景，比如 RTC 场景。只是说如果你非要用这种普通 I 帧，标准也是支持的。

GOP

在 H264 中，还有一个 GOP 的概念也经常会遇到，它是什么意思呢？从一个 IDR 帧开始到下一个 IDR 帧的前一帧为止，这里面包含的 IDR 帧、普通 I 帧、P 帧和 B 帧，我们称为一个 **GOP（图像组）**（这是 closed GOP，还有一种 opened GOP，比较少见，这里不讨论）。

我们可以看到 GOP 的大小是由 IDR 帧之间的间隔来确定的，而这个间隔我们有一个重要的概念来表示，叫做**关键帧间隔。**关键帧间隔越大，两个 IDR 相隔就会越远，GOP 也就越大；关键帧间隔越小，IDR 相隔也就越近，GOP 就越小。

![img](https://static001.geekbang.org/resource/image/3d/df/3deac858fff85f0bf3d9c930e6776cdf.jpeg?wh=1920x416)

GOP 越大，编码的 I 帧就会越少。相比而言，P 帧、B 帧的压缩率更高，因此整个视频的编码效率就会越高。但是 GOP 太大，也会导致 IDR 帧距离太大，点播场景时进行视频的 seek 操作就会不方便。

并且，在 RTC 和直播场景中，可能会因为网络原因导致丢包而引起接收端的丢帧，大的 GOP 最终可能导致参考帧丢失而出现解码错误，从而引起长时间花屏和卡顿。这一块我们会在之后用单独的一节课来详细讲述。总之，**GOP 不是越大越好，也不是越小越好，需要根据实际的场景来选择。**

GOP 越大，编码的 I 帧就会越少。相比而言，P 帧、B 帧的压缩率更高，因此整个视频的编码效率就会越高。但是 GOP 太大，也会导致 IDR 帧距离太大，点播场景时进行视频的 seek 操作就会不方便。并且，在 RTC 和直播场景中，可能会因为网络原因导致丢包而引起接收端的丢帧，大的 GOP 最终可能导致参考帧丢失而出现解码错误，从而引起长时间花屏和卡顿。这一块我们会在之后用单独的一节课来详细讲述。总之，GOP 不是越大越好，也不是越小越好，需要根据实际的场景来选择。

前面我们讲的是视频图像序列的层次结构，那图像内的层次结构是怎样的呢？

Slice

这就不得不提到另一个概念了，Slice，也叫做“片”。Slice 其实是为了并行编码设计的。什么意思呢？就是说，我们可以将一帧图像划分成几个 Slice，并且 Slice 之间相互独立、互不依赖、独立编码。

那么在机器性能比较高的情况下，我们就可以多线程并行对多个 Slice 进行编码，从而提升速度。但也因为一帧内的几个 Slice 是相互独立的，所以如果帧内预测的话，就不能跨 Slice 进行，因此编码性能会差一些。

而在 H264 中编码的基本单元是宏块，所以一个 Slice 又包含整数个宏块。我们在前一节课中也讲了，宏块 MB 大小是 16 x 16。在做帧内和帧间预测的时候，我们又可以将宏块继续划分成不同大小的子块，用来给复杂区域做精细化编码。

总结来说，图像内的层次结构就是一帧图像可以划分成一个或多个 Slice，而一个 Slice 包含多个宏块，且一个宏块又可以划分成多个不同尺寸的子块。如下图所示

![img](https://static001.geekbang.org/resource/image/63/16/63f316bf5d1410cdb38334ba7bc9f316.jpg?wh=1280x720)

好了，上面都是从概念上来讨论视频编码中的视频序列和图像的层次结构。那有了这些知识之后，接下来我们更进一步，从 H264 码流的角度来看看这些层次结构具体在二进制码流中是怎样的。

H264 的码流结构

下面我们就以“剥洋葱”的方式来详细地讲解 H264 的码流结构。先从最外层的码流格式讲起，教你怎么判断视频编码数据的起始；然后再介绍里面的 NALU（网络抽象层单元）数据，看看通过它是怎么区分不同的帧类型的；再详细聊聊 NALU 有几种类型，以及通过什么方式来区分 NALU 的类型。

码流格式H264 码流有两种格式：一种是 Annexb 格式；一种是 MP4 格式。两种格式的区别是：

1. Annexb 格式使用**起始码**来表示一个编码数据的开始。起始码本身不是图像编码的内容，只是用来分隔用的。起始码有两种，一种是 4 字节的“00 00 00 01”，一种是 3 字节的“00 00 01”。

这里需要**注意**一下，由于图像编码出来的数据中也有可能出现“00 00 00 01”和“00 00 01”的数据。那这种情况怎么办呢？为了防止出现这种情况，H264 会将图像编码数据中的下面的几种字节串做如下处理：

（1）“00 00 00”修改为“00 00 03 00”；

（2）“00 00 01”修改为“00 00 03 01”；

（3）“00 00 02”修改为“00 00 03 02”；

（4）“00 00 03”修改为“00 00 03 03”。

同样地在解码端，我们在去掉起始码之后，也需要将对应的字节串转换回来。

![img](https://static001.geekbang.org/resource/image/8f/b2/8f7b9f988aa439d8ae584eb4561ed1b2.jpeg?wh=1920x311)

2. MP4 格式没有起始码，而是在图像编码数据的开始使用了 4 个字节作为长度标识，用来表示编码数据的长度，这样我们每次读取 4 个字节，计算出编码数据长度，然后取出编码数据，再继续读取 4 个字节得到长度，一直继续下去就可以取出所有的编码数据了。

![img](https://static001.geekbang.org/resource/image/94/6d/945b7e9c21e313c1cfcd8ecbe967576d.jpeg?wh=1920x250)

这两种格式差别不大，接下来我们主要使用 Annexb 格式来讲解 H264 码流中的 NALU。

下面，我们剥开“洋葱”的最外层，将起始码去掉，进入“洋葱”的内部，也就是编码数据。这个编码数据就是 H264 码流的重要部分——NALU。









